{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Import libraries"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pydot as pydot\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import plotly.express as px\n",
    "plt.style.use(\"ggplot\")\n",
    "rcParams['figure.figsize'] = (12,6)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import brier_score_loss\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import tree\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras import initializers\n",
    "from tensorflow import keras\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import StratifiedKFold, GridSearchCV\n",
    "from imblearn import over_sampling\n",
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate, learning_curve, cross_val_predict\n",
    "import keras_tuner as kt\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define functions"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def hypertune_threshold(model, start, stop, X_test, y_test, pos, neg):\n",
    "    threshold = np.linspace(start = start, stop = stop, num = 100)\n",
    "    f1 = np.zeros(threshold.size)\n",
    "    predictions = model.predict_proba(X_test)\n",
    "    for i in np.arange(threshold.size):\n",
    "        probs = np.where(predictions[:,pos]>=threshold[i], pos, neg)\n",
    "        f1[i] = f1_score(np.ravel(y_test), probs, pos_label=pos)\n",
    "    final_probs = np.where(predictions[:,pos]>=threshold[np.argmax(f1)], pos, neg)\n",
    "    return print(threshold[np.argmax(f1)], f1_score(np.ravel(y_test), final_probs, pos_label=pos)), final_probs\n",
    "\n",
    "def idx_cm(y_pred, y_test, pos, neg):\n",
    "    fp_rows = []\n",
    "    fn_rows = []\n",
    "    tp_rows = []\n",
    "    tn_rows = []\n",
    "\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] == pos and y_test[i] == neg:\n",
    "            fp_rows.append(i)\n",
    "        elif y_pred[i] == pos and y_test[i] == pos:\n",
    "            tp_rows.append(i)\n",
    "        elif y_pred[i] == neg and y_test[i] == pos:\n",
    "            fn_rows.append(i)\n",
    "        elif y_pred[i] == neg and y_test[i] == neg:\n",
    "            tn_rows.append(i)\n",
    "    fp_rows = pd.DataFrame(fp_rows)\n",
    "    tp_rows = pd.DataFrame(tp_rows)\n",
    "    fn_rows = pd.DataFrame(fn_rows)\n",
    "    tn_rows = pd.DataFrame(tn_rows)\n",
    "    cm = [fp_rows, tp_rows, fn_rows, tn_rows]\n",
    "    return cm\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read in data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = pd.read_pickle('x_train_lvl1.pkl')\n",
    "X_test = pd.read_pickle('x_test_lvl1.pkl')\n",
    "y_train = np.ravel(pd.read_pickle('y_train_lvl1.pkl'))\n",
    "y_test = np.ravel(pd.read_pickle('y_test_lvl1.pkl'))\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape, X_train.info())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_train = ~y_train+2\n",
    "y_test = ~y_test+2\n",
    "\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "\n",
    "scaler = preprocessing.MinMaxScaler((0, 1)) # normalize data for neural network and logistic regression\n",
    "X_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X_scaled, y_train)\n",
    "\n",
    "print('Training labels shape:', y_resampled.shape)\n",
    "\n",
    "print('Training features shape:', X_resampled.shape)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LR = LogisticRegression(solver='newton-cholesky')\n",
    "RF = RandomForestClassifier(max_depth=10, min_samples_leaf=10, min_samples_split=5, n_estimators=1250,random_state=1)\n",
    "\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_auc',\n",
    "    verbose=1,\n",
    "    patience=10,\n",
    "    mode='max',\n",
    "    restore_best_weights=True)\n",
    "\n",
    "\n",
    "METRICS = [\n",
    "      keras.metrics.AUC(name='auc')\n",
    "]\n",
    "\n",
    "\n",
    "NN = Sequential()\n",
    "NN.add(Dense(256, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "NN.add(Dropout(0.2))\n",
    "NN.add(Dense(112, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "NN.add(Dropout(0.2))\n",
    "NN.add(Dense(1, activation='sigmoid'))\n",
    "# Compile model\n",
    "NN.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=\"keras.metrics.auc\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train and evaluate models using cv\n",
    "The following code represents the neural network. For logistic regression and LR the model is changed."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the maximum train set size and the number of folds\n",
    "max_train_size = len(X_train)\n",
    "n_folds = 5\n",
    "\n",
    "# Initialize lists to store train set sizes and evaluation scores for each fold\n",
    "train_sizes = []\n",
    "auc_roc_scores_test = []\n",
    "auc_roc_scores_train = []\n",
    "f1_scores_test = []\n",
    "f1_scores_train = []\n",
    "brier_scores_test = []\n",
    "brier_scores_train = []\n",
    "loss_scores_test = []\n",
    "loss_scores_train = []\n",
    "\n",
    "# Perform cross-validation on different train set sizes\n",
    "for train_size in range(500, max_train_size+1, 1000):\n",
    "    # Initialize the cross-validation object\n",
    "    skf = StratifiedKFold(n_splits=n_folds)\n",
    "\n",
    "    fold_auc_roc_scores_test = []\n",
    "    fold_f1_scores_test = []\n",
    "    fold_brier_scores_test = []\n",
    "    fold_loss_scores_test = []\n",
    "    fold_auc_roc_scores_train = []\n",
    "    fold_f1_scores_train = []\n",
    "    fold_brier_scores_train = []\n",
    "    fold_loss_scores_train = []\n",
    "\n",
    "    # Split the data into train and test sets using stratified cross-validation\n",
    "    for train_index, test_index in skf.split(X_train[:train_size], y_train[:train_size]):\n",
    "        X_train_fold, X_test_fold = X_train[:train_size][train_index], X_train[:train_size][test_index]\n",
    "        y_train_fold, y_test_fold = y_train[:train_size][train_index], y_train[:train_size][test_index]\n",
    "\n",
    "        # Normalize the data within the fold\n",
    "        scaler = preprocessing.MinMaxScaler((0,1))\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_test_fold = scaler.transform(X_test_fold)\n",
    "\n",
    "        # Resample\n",
    "        ros = RandomUnderSampler(random_state=42)\n",
    "        train_resampled, y_resampled = ros.fit_resample(X_train_fold, y_train_fold)\n",
    "        # Define and compile the neural network model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(256, kernel_initializer = initializers.RandomNormal(stddev=0.01), input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(112, input_shape=(X_train.shape[-1],), activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        # Compile model\n",
    "        model.compile(loss='binary_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=1e-5), metrics=METRICS)\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(train_resampled, y_resampled, epochs = 100, batch_size=2048, validation_data=(X_test_fold, y_test_fold), callbacks=[early_stopping])\n",
    "\n",
    "        # Make predictions on the test set\n",
    "        y_pred_test = model.predict(X_test_fold)\n",
    "        y_pred_train = model.predict(X_train_fold)\n",
    "\n",
    "        # Calculate and store the AUC-ROC score\n",
    "        auc_roc_test = roc_auc_score(y_test_fold, y_pred_test)\n",
    "        auc_roc_train = roc_auc_score(y_train_fold, y_pred_train)\n",
    "        fold_auc_roc_scores_test.append(auc_roc_test)\n",
    "        fold_auc_roc_scores_train.append(auc_roc_train)\n",
    "\n",
    "        # Calculate and store the F1 score\n",
    "        y_pred_binary_test = np.round(y_pred_test)\n",
    "        y_pred_binary_train = np.round(y_pred_train)\n",
    "        f1_test = f1_score(y_test_fold, y_pred_binary_test)\n",
    "        f1_train = f1_score(y_train_fold, y_pred_binary_train)\n",
    "        fold_f1_scores_test.append(f1_test)\n",
    "        fold_f1_scores_train.append(f1_train)\n",
    "\n",
    "        # Calculate and store the Brier loss\n",
    "        brier_loss_test = brier_score_loss(y_test_fold, y_pred_test)\n",
    "        fold_brier_scores_test.append(brier_loss_test)\n",
    "        brier_loss_train = brier_score_loss(y_train_fold, y_pred_train)\n",
    "        fold_brier_scores_train.append(brier_loss_train)\n",
    "\n",
    "        # Calculate and store the loss\n",
    "        loss_test = log_loss(y_test_fold, y_pred_test)\n",
    "        fold_loss_scores_test.append(loss_test)\n",
    "        loss_train = log_loss(y_train_fold, y_pred_train)\n",
    "        fold_loss_scores_train.append(loss_train)\n",
    "\n",
    "    # Calculate the average scores across all folds for the current train set size\n",
    "    average_auc_roc_test = np.mean(fold_auc_roc_scores_test)\n",
    "    average_f1_test = np.mean(fold_f1_scores_test)\n",
    "    average_brier_test = np.mean(fold_brier_scores_test)\n",
    "    average_loss_test = np.mean(fold_loss_scores_test)\n",
    "    average_auc_roc_train = np.mean(fold_auc_roc_scores_train)\n",
    "    average_f1_train = np.mean(fold_f1_scores_train)\n",
    "    average_brier_train = np.mean(fold_brier_scores_train)\n",
    "    average_loss_train = np.mean(fold_loss_scores_train)\n",
    "\n",
    "    # Append the train set size and average scores to the respective lists\n",
    "    train_sizes.append(train_size)\n",
    "    auc_roc_scores_test.append(average_auc_roc_test)\n",
    "    f1_scores_test.append(average_f1_test)\n",
    "    brier_scores_test.append(average_brier_test)\n",
    "    loss_scores_test.append(average_loss_test)\n",
    "    auc_roc_scores_train.append(average_auc_roc_train)\n",
    "    f1_scores_train.append(average_f1_train)\n",
    "    brier_scores_train.append(average_brier_train)\n",
    "    loss_scores_train.append(average_loss_train)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot training curves - training size vs loss & training size vs f1 & training size vs roc_auc & training size vs brier"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax= sns.lineplot(x=train_sizes, y=auc_roc_scores_test, label = \"test\")\n",
    "ax1 = sns.lineplot(x=train_sizes, y=auc_roc_scores_train, label = \"train\")\n",
    "ax.set(xlabel='Number of observations', ylabel='ROC_AUC')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax = plt.semilogy(train_sizes, auc_roc_scores_test, label=\"test\")\n",
    "ax1 = plt.semilogy(train_sizes, auc_roc_scores_train, label=\"train\")\n",
    "\n",
    "plt.xlabel('Number of observations')\n",
    "plt.legend()\n",
    "# Give y axis label for the semilogy plot\n",
    "plt.ylabel('ROC_AUC')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax= sns.lineplot(x=train_sizes, y=f1_scores_test, label = \"test\")\n",
    "ax1 = sns.lineplot(x=train_sizes, y=f1_scores_train, label = \"train\")\n",
    "ax.set(xlabel='Number of observations', ylabel='F1')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax= sns.lineplot(x=train_sizes, y=loss_scores_test, label = \"test\")\n",
    "ax1 = sns.lineplot(x=train_sizes, y=loss_scores_train, label = \"train\")\n",
    "ax.set(xlabel='Number of observations', ylabel='Logloss')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax= sns.lineplot(x=train_sizes, y=brier_scores_test, label = \"test\")\n",
    "ax1 = sns.lineplot(x=train_sizes, y=brier_scores_train, label = \"train\")\n",
    "ax.set(xlabel='Number of observations', ylabel='Brier score')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Identify false negatives and false positives and save indices for comparison between models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NN.fit(X_resampled, y_resampled, epochs=1000, batch_size=2048, verbose=2, validation_data = (X_test_scaled, y_test), callbacks = [early_stopping])\n",
    "LR_fitted = LR.fit(X_resampled, y_resampled)\n",
    "RF_fitted = RF.fit(X_resampled, y_resampled)\n",
    "test_predictions_baseline = NN.predict(X_test_scaled, batch_size=2048)\n",
    "yhat = np.where(test_predictions_baseline > 0.5, 1, 0)  #predictions for greatest class\n",
    "cm_LR = idx_cm(LR_fitted.predict(X_test_scaled), y_test, 1, 0)\n",
    "cm_RF = idx_cm(RF_fitted.predict(X_test_scaled), y_test, 1, 0)\n",
    "cm_NN = idx_cm(yhat, y_test, 1, 0)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(cm_RF[0].shape, cm_RF[1].shape, cm_RF[2].shape, cm_RF[3].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(cm_NN[0].shape, cm_NN[1].shape, cm_NN[2].shape, cm_NN[3].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(cm_LR[0].shape, cm_LR[1].shape, cm_LR[2].shape, cm_LR[3].shape)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "cm_list = [cm_LR, cm_RF, cm_NN]\n",
    "\n",
    "def inner_join_cm(cm_list): #cm_model needs to be a list with\n",
    "    inner_all = []\n",
    "    for model1, model2 in itertools.combinations(cm_list, 2):\n",
    "        inner = np.zeros((len(cm_list[0])))\n",
    "        for i in range(len(cm_list[0])):\n",
    "            inner[i] = pd.merge(model1[i],model2[i], how=\"inner\", indicator=\"True\").shape[0]\n",
    "        inner_all.append(inner)\n",
    "    return inner_all\n",
    "\n",
    "def left_join_cm(cm_list):\n",
    "    left_all = []\n",
    "    for model1, model2 in itertools.combinations(cm_list, 2):\n",
    "        left = []\n",
    "        for i in range(len(cm_list[0])):\n",
    "            left = pd.merge(model1[i],model2[i], how=\"left\", indicator=\"True\")\n",
    "            idx = left[left[\"True\"]==\"left_only\"]\n",
    "            left_all.append(idx)\n",
    "    return left_all"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "inner_join_cm(cm_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape) # 162.757 --> only about 10.000 observations that the models disagree on"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot ROC curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fpr, tpr, _ = metrics.roc_curve(y_test, model.predict_proba(X_test_scaled)[:,1])\n",
    "\n",
    "#create ROC curve\n",
    "plt.plot(fpr,tpr)\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot Precision recall curves"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, model.predict_proba(X_test_scaled)[:, 1])\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(recall, precision, color='blue')\n",
    "\n",
    "#add axis labels to plot\n",
    "ax.set_title('Precision-Recall Curve')\n",
    "ax.set_ylabel('Precision')\n",
    "ax.set_xlabel('Recall')\n",
    "\n",
    "#display plot\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
